# 5. Implementation Tasks

This document breaks down the development work into a series of concrete tasks, based on the architecture and patterns we've defined.

## Epic 1: Project Setup & Configuration

-   [ ] **Task 1.1: Create Project Structure:**
    -   Create a `src` directory for the main application code.
    -   Inside `src`, create directories for each of our main components: `config`, `scraper`, `agents`, and `notifier`.
    -   Create an empty `__init__.py` file in each new directory to mark them as Python packages.
-   [ ] **Task 1.2: Implement Config Manager:**
    -   Create `src/config/config.py`.
    -   Implement the `Config` class using the Singleton pattern.
    -   Add methods to load and provide data from:
        -   `.env` file (for API keys). We will use the `python-dotenv` library.
        -   `search_urls.txt`.
        -   `cookies.json`.
        -   `resume.json`.
        -   `ideal_job_profile.txt`.
        -   Files within the `writing_style_samples/` directory.
-   [ ] **Task 1.3: Create Placeholder Input Files:**
    -   Create empty or example versions of all the user-facing configuration files (`search_urls.txt`, `resume.json`, etc.) in the project root. This makes it clear to the user what they need to provide.

## Epic 2: Core Scraping Logic

-   [ ] **Task 2.1: Implement Scraper Base Class:**
    -   Create `src/scraper/base.py`.
    -   Define an abstract base class `Scraper` with an abstract method `scrape()`.
-   [ ] **Task 2.2: Implement LinkedIn Scraper:**
    -   Create `src/scraper/linkedin_scraper.py`.
    -   Implement the `LinkedInScraper` class, inheriting from `Scraper`.
    -   The `__init__` method should take the configuration object.
    -   The `scrape` method will contain the core Selenium logic:
        -   Initialize WebDriver.
        -   Load cookies.
        -   Iterate through search URLs.
        -   Extract job links from each search page.
        -   Visit each job link and extract the full description text.
        -   Return a list of `Job` data objects.
-   [ ] **Task 2.3: Define Job Data Structure:**
    -   Create a `Job` dataclass or Pydantic model to standardize the structure of scraped job data (title, company, url, description).

## Epic 3: Agentic Workflow (ADK)

-   [ ] **Task 3.1: Set up ADK Environment:**
    -   Ensure `google-adk` is listed in our `pyproject.toml` or `requirements.txt`.
    -   Create `src/agents/workflow.py` to house the main ADK orchestration logic.
-   [ ] **Task 3.2: Implement Job Validation Agent:**
    -   Create a new agent function/class.
    -   It will take a list of `Job` objects and the `ideal_job_profile` text as input.
    -   It will use an LLM to compare each job to the profile and filter the list, returning only the qualified jobs.
-   [ ] **Task 3.3: Implement Content Generation Agent:**
    -   Create a new agent function/class.
    -   It will take a single `Job` object, the `resume.json` data, and the writing style samples as input.
    -   It will use an LLM to generate the suggested bullet point changes and the cover letter.
    -   It will return a structured object containing the generated content.
-   [ ] **Task 3.4: Implement Content Review Agent:**
    -   Create a new agent function/class.
    -   It will take the output from the generation agent and the original job description.
    -   It will use an LLM to score the quality and relevance of the generated content.
    -   It will return a score and feedback.
-   [ ] **Task 3.5: Orchestrate Agents in a Sequence:**
    -   In `src/agents/workflow.py`, use the ADK `Sequential` agent to chain the three agents together, including the review/retry loop logic.

## Epic 4: Notification & Orchestration

-   [ ] **Task 4.1: Implement Telegram Notifier:**
    -   Create `src/notifier/telegram_notifier.py`.
    -   Implement the `TelegramNotifier` class using the Adapter pattern.
    -   It will have a single method, `send(message)`, which makes the `POST` request to the Telegram API.
-   [ ] **Task 4.2: Implement the Main Orchestrator:**
    -   Create `src/main.py`.
    -   This script will implement the main pipeline logic:
        1.  Initialize Config.
        2.  Initialize and run the Scraper.
        3.  Pass the results to the ADK workflow.
        4.  Receive the final messages from the workflow.
        5.  Initialize the Notifier and send the messages.
-   [ ] **Task 4.3: Add Logging:**
    -   Implement structured logging throughout the application to provide visibility into the process. We will use Python's built-in `logging` module.

## Epic 5: Finalization

-   [ ] **Task 5.1: Create `README.md`:**
    -   Write a comprehensive `README.md` for the project, explaining what it is, how to set it up (including how to generate the `cookies.json`), and how to run it.
-   [ ] **Task 5.2: Create `requirements.txt`:**
    -   Pin all project dependencies (e.g., `selenium`, `google-adk`, `python-dotenv`, `requests`) in a `requirements.txt` file for reproducible installs.
-   [ ] **Task 5.3: Write Manual Cookie Generation Script:**
    -   Create a separate, standalone Python script (`generate_cookies.py`) that uses Selenium to:
        1.  Open LinkedIn's login page.
        2.  Wait for the user to log in manually.
        3.  Once the user is logged in, export the session cookies to `cookies.json`.
        4.  Provide clear instructions to the user in the console. 